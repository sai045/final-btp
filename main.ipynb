{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sai045/final-btp/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "imagePatches = glob.glob('archive/8863/*/*')\n",
    "imagePatches = [imagePatches[i] for i in range(len(imagePatches)) if 'IDC' not in imagePatches[i]]\n",
    "y = []\n",
    "for img in imagePatches:\n",
    "    if img.endswith('class0.png'):\n",
    "        y.append(0)\n",
    "    elif img.endswith('class1.png'):\n",
    "        y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Create custom dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df_data, transform=None):\n",
    "        super().__init__()\n",
    "        self.df = df_data.values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, label = self.df[index]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.resize(image, (50, 50))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "images_df = pd.DataFrame()\n",
    "images_df[\"images\"] = imagePatches\n",
    "images_df[\"labels\"] = y\n",
    "train, test = train_test_split(images_df, stratify=images_df.labels, test_size=0.2, random_state=42)\n",
    "train, val = train_test_split(train, stratify=train.labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              images  labels\n",
      "751   archive/8863/0/8863_idx5_x851_y1651_class0.png       0\n",
      "34   archive/8863/1/8863_idx5_x1301_y1451_class1.png       1\n",
      "132   archive/8863/1/8863_idx5_x1151_y751_class1.png       1\n",
      "76   archive/8863/1/8863_idx5_x1501_y1051_class1.png       1\n",
      "266  archive/8863/0/8863_idx5_x1651_y2301_class0.png       0\n",
      "..                                               ...     ...\n",
      "320  archive/8863/0/8863_idx5_x1901_y1851_class0.png       0\n",
      "488   archive/8863/0/8863_idx5_x651_y1301_class0.png       0\n",
      "701  archive/8863/0/8863_idx5_x1251_y1401_class0.png       0\n",
      "525   archive/8863/0/8863_idx5_x501_y1451_class0.png       0\n",
      "268   archive/8863/0/8863_idx5_x201_y1051_class0.png       0\n",
      "\n",
      "[626 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "num_classes = 2\n",
    "batch_size = 128\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "trans_train = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor()])\n",
    "trans_valid = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "dataset_train = MyDataset(df_data=train, transform=trans_train)\n",
    "dataset_valid = MyDataset(df_data=val, transform=trans_valid)\n",
    "dataset_test = MyDataset(df_data=test, transform=trans_valid)\n",
    "loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "loader_valid = DataLoader(dataset=dataset_valid, batch_size=batch_size//2, shuffle=True, num_workers=0)\n",
    "loader_test = DataLoader(dataset=dataset_test, batch_size=batch_size//2, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Load pre-trained VGG16 model\n",
    "vggmodel = models.vgg16(weights='IMAGENET1K_V1')\n",
    "vggmodel.classifier[6] = nn.Linear(4096, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Freeze pre-trained layers\n",
    "for n, p in vggmodel.named_parameters():\n",
    "    if 'classifier' in n:\n",
    "        pass\n",
    "    else:\n",
    "        p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(vggmodel.classifier.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model to device\n",
    "vggmodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "vgg_best_accuracy = 0\n",
    "vgg_best_weights = None\n",
    "trl, trac, vall, valac = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 0.5824, Train Acc: 0.7188, Val Loss: 0.4819, Val Acc: 0.8025\n",
      "Epoch 2 - Train Loss: 0.4302, Train Acc: 0.8323, Val Loss: 0.5405, Val Acc: 0.7962\n",
      "Epoch 3 - Train Loss: 0.4451, Train Acc: 0.8482, Val Loss: 0.4367, Val Acc: 0.8280\n",
      "Epoch 4 - Train Loss: 0.3536, Train Acc: 0.8626, Val Loss: 0.3613, Val Acc: 0.8471\n",
      "Epoch 5 - Train Loss: 0.3145, Train Acc: 0.8706, Val Loss: 0.3761, Val Acc: 0.8408\n",
      "Epoch 6 - Train Loss: 0.3086, Train Acc: 0.8738, Val Loss: 0.3374, Val Acc: 0.8726\n",
      "Epoch 7 - Train Loss: 0.2842, Train Acc: 0.8722, Val Loss: 0.3160, Val Acc: 0.8726\n",
      "Epoch 8 - Train Loss: 0.2763, Train Acc: 0.8818, Val Loss: 0.3330, Val Acc: 0.8790\n",
      "Epoch 9 - Train Loss: 0.2759, Train Acc: 0.8834, Val Loss: 0.3406, Val Acc: 0.8790\n",
      "Epoch 10 - Train Loss: 0.2584, Train Acc: 0.8946, Val Loss: 0.3274, Val Acc: 0.8599\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    vggmodel.train()\n",
    "    for i, (inputs, targets) in enumerate(loader_train):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vggmodel(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        train_correct += (predicted == targets).sum().item()\n",
    "        train_total += targets.size(0)\n",
    "    train_loss /= len(train)\n",
    "    train_acc = train_correct / train_total\n",
    "    trl.append(train_loss)\n",
    "    trac.append(train_acc)\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    vggmodel.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader_valid:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = vggmodel(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            val_correct += (predicted == targets).sum().item()\n",
    "            val_total += targets.size(0) \n",
    "    val_loss /= len(val)\n",
    "    val_acc = val_correct / val_total\n",
    "    vall.append(val_loss)\n",
    "    valac.append(val_acc)\n",
    "\n",
    "    if val_acc > vgg_best_accuracy:\n",
    "        vgg_best_accuracy = val_acc\n",
    "        vgg_best_weights = vggmodel.state_dict()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the test images: 90.3061224489796 %\n",
      "[[146   9]\n",
      " [ 10  31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94       155\n",
      "           1       0.78      0.76      0.77        41\n",
      "\n",
      "    accuracy                           0.90       196\n",
      "   macro avg       0.86      0.85      0.85       196\n",
      "weighted avg       0.90      0.90      0.90       196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cuda_tensor = torch.tensor(vall)\n",
    "vls = cuda_tensor.cpu()\n",
    "cuda_tensor = torch.tensor(trl)\n",
    "tls = cuda_tensor.cpu()\n",
    "\n",
    "vggmodel.load_state_dict(vgg_best_weights)\n",
    "vggmodel.to(device)\n",
    "\n",
    "vggpredict = []\n",
    "vgglabel = []\n",
    "\n",
    "vggmodel.eval()\n",
    "confusion_matrix = torch.zeros(2, 2)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loader_test:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = vggmodel(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        vggpredict.extend(predicted)\n",
    "        vgglabel.extend(labels)\n",
    "\n",
    "        for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "    print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "label_vgg = [tensor.cpu().numpy() for tensor in vgglabel]\n",
    "vgg_array = [tensor.cpu().numpy() for tensor in vggpredict]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(label_vgg, vgg_array))\n",
    "print(classification_report(label_vgg, vgg_array))\n",
    "\n",
    "dfv = pd.DataFrame()\n",
    "dfv[\"vgg\"] = vgg_array\n",
    "dfv[\"label\"] = label_vgg\n",
    "dfv.head()\n",
    "dfv.to_csv('vgwithaug.csv')\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': vggmodel.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, 'checkpointvgg50withaug.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Adversarial attack functions\n",
    "def fgsm_attack(model, criterion, images, labels, epsilon):\n",
    "    model.eval()\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    images.requires_grad = True\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    gradient_sign = torch.sign(images.grad)\n",
    "    perturbed_images = images + epsilon * gradient_sign\n",
    "    perturbed_images = torch.clamp(perturbed_images, 0, 1)\n",
    "    return perturbed_images\n",
    "\n",
    "def pgd_attack(model, criterion, images, labels, epsilon, alpha, iters):\n",
    "    model.eval()\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    perturbed_images = images.clone().detach().requires_grad_(True)\n",
    "    for i in range(iters):\n",
    "        outputs = model(perturbed_images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        model.zero_grad()\n",
    "        gradients = autograd.grad(loss, perturbed_images)[0]\n",
    "        gradient_sign = torch.sign(gradients)\n",
    "        perturbed_images = perturbed_images + alpha * gradient_sign\n",
    "        perturbed_images = torch.max(torch.min(perturbed_images, images + epsilon), images - epsilon)\n",
    "        perturbed_images = torch.clamp(perturbed_images, 0, 1)\n",
    "    return perturbed_images\n",
    "\n",
    "def ifgsm_attack(model, criterion, images, labels, epsilon, alpha, iters):\n",
    "    model.eval()\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    perturbed_images = images.clone().detach().requires_grad_(True)\n",
    "    for i in range(iters):\n",
    "        outputs = model(perturbed_images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        model.zero_grad()\n",
    "        gradients = autograd.grad(loss, perturbed_images)[0]\n",
    "        gradient_sign = torch.sign(gradients)\n",
    "        perturbed_images = perturbed_images + alpha * gradient_sign\n",
    "        perturbed_images = torch.max(torch.min(perturbed_images, images + epsilon), images - epsilon)\n",
    "        perturbed_images = torch.clamp(perturbed_images, 0, 1)\n",
    "    return perturbed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Create a directory to save the adversarial examples\n",
    "adv_examples_dir = 'adv_examples'\n",
    "os.makedirs(adv_examples_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Function to save adversarial examples as image files\n",
    "def save_adv_examples(adv_images, labels, attack_name):\n",
    "    attack_dir = os.path.join(adv_examples_dir, attack_name)\n",
    "    os.makedirs(attack_dir, exist_ok=True)\n",
    "\n",
    "    for i, (adv_img, label) in enumerate(zip(adv_images, labels)):\n",
    "        # Convert the adversarial image tensor to a PIL Image\n",
    "        adv_img = adv_img.permute(1, 2, 0).cpu().detach().numpy()\n",
    "        adv_img = (adv_img * 255).astype('uint8')\n",
    "        adv_img = Image.fromarray(adv_img)\n",
    "\n",
    "        # Save the adversarial image as a PNG file\n",
    "        file_name = f\"{label}_{i}.png\"\n",
    "        adv_img.save(os.path.join(attack_dir, file_name))\n",
    "\n",
    "    print(f'Adversarial examples from {attack_name} attack saved to {attack_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on FGSM perturbed images: 50.78%\n",
      "Adversarial examples from fgsm attack saved to adv_examples/fgsm\n"
     ]
    }
   ],
   "source": [
    "# Save FGSM adversarial examples\n",
    "fgsm_images, fgsm_labels = next(iter(loader_train))\n",
    "fgsm_adv_images = fgsm_attack(vggmodel, criterion, fgsm_images, fgsm_labels, epsilon=0.05)\n",
    "\n",
    "# Evaluate the model's predictions on the perturbed images\n",
    "vggmodel.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = vggmodel(fgsm_adv_images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "# Move fgsm_labels tensor to the same device as predicted tensor\n",
    "fgsm_labels = fgsm_labels.to(predicted.device)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (predicted == fgsm_labels).sum().item() / fgsm_labels.size(0)\n",
    "print(f\"Accuracy on FGSM perturbed images: {accuracy:.2%}\")\n",
    "\n",
    "save_adv_examples(fgsm_adv_images, fgsm_labels, 'fgsm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on PGD perturbed images: 1.56%\n",
      "Adversarial examples from pgd attack saved to adv_examples/pgd\n"
     ]
    }
   ],
   "source": [
    "# Save PGD adversarial examples\n",
    "pgd_images, pgd_labels = next(iter(loader_train))\n",
    "pgd_adv_images = pgd_attack(vggmodel, criterion, pgd_images, pgd_labels, epsilon=0.05, alpha=0.01, iters=10)\n",
    "\n",
    "# Evaluate the model's predictions on the perturbed images\n",
    "vggmodel.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = vggmodel(pgd_adv_images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "# Move fgsm_labels tensor to the same device as predicted tensor\n",
    "pgd_labels = pgd_labels.to(predicted.device)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (predicted == pgd_labels).sum().item() / pgd_labels.size(0)\n",
    "print(f\"Accuracy on PGD perturbed images: {accuracy:.2%}\")\n",
    "\n",
    "save_adv_examples(pgd_adv_images, pgd_labels, 'pgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on I-FGSM perturbed images: 0.78%\n",
      "Adversarial examples from ifgsm attack saved to adv_examples/ifgsm\n"
     ]
    }
   ],
   "source": [
    "# Save I-FGSM adversarial examples\n",
    "ifgsm_images, ifgsm_labels = next(iter(loader_train))\n",
    "ifgsm_adv_images = ifgsm_attack(vggmodel, criterion, ifgsm_images, ifgsm_labels, epsilon=0.05, alpha=0.01, iters=10)\n",
    "\n",
    "# Evaluate the model's predictions on the perturbed images\n",
    "vggmodel.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = vggmodel(ifgsm_adv_images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "# Move fgsm_labels tensor to the same device as predicted tensor\n",
    "ifgsm_labels = ifgsm_labels.to(predicted.device)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (predicted == ifgsm_labels).sum().item() / ifgsm_labels.size(0)\n",
    "print(f\"Accuracy on I-FGSM perturbed images: {accuracy:.2%}\")\n",
    "\n",
    "\n",
    "save_adv_examples(ifgsm_adv_images, ifgsm_labels, 'ifgsm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "all_image_patches = glob.glob('adv_examples/*/*')\n",
    "new_y = []\n",
    "for img in all_image_patches:\n",
    "    imgName = img.split(\"/\")[-1]\n",
    "    if imgName.startswith('0_'):\n",
    "        new_y.append(0)\n",
    "    elif imgName.startswith('1_'):\n",
    "        new_y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "imagePatches = glob.glob('dataset/8863/*/*')\n",
    "imagePatches = [imagePatches[i] for i in range(len(imagePatches)) if 'IDC' not in imagePatches[i]]\n",
    "for img in imagePatches:\n",
    "    all_image_patches.append(img)\n",
    "    if img.endswith('class0.png'):\n",
    "        new_y.append(0)\n",
    "    elif img.endswith('class1.png'):\n",
    "        new_y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "516\n"
     ]
    }
   ],
   "source": [
    "print(len(all_image_patches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "516\n"
     ]
    }
   ],
   "source": [
    "print(len(new_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "images_df = pd.DataFrame()\n",
    "images_df[\"images\"] = all_image_patches\n",
    "images_df[\"labels\"] = new_y\n",
    "train, test = train_test_split(images_df, stratify=images_df.labels, test_size=0.2, random_state=42)\n",
    "train, val = train_test_split(train, stratify=train.labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "dataset_train = MyDataset(df_data=train, transform=trans_train)\n",
    "dataset_valid = MyDataset(df_data=val, transform=trans_valid)\n",
    "dataset_test = MyDataset(df_data=test, transform=trans_valid)\n",
    "loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "loader_valid = DataLoader(dataset=dataset_valid, batch_size=batch_size//2, shuffle=True, num_workers=0)\n",
    "loader_test = DataLoader(dataset=dataset_test, batch_size=batch_size//2, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset the model and optimizer\n",
    "vggmodel = models.vgg16(weights='IMAGENET1K_V1')\n",
    "vggmodel.classifier[6] = nn.Linear(4096, num_classes)\n",
    "for n, p in vggmodel.named_parameters():\n",
    "    if 'classifier' in n:\n",
    "        pass\n",
    "    else:\n",
    "        p.requires_grad = False\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(vggmodel.classifier.parameters(), lr=learning_rate, momentum=0.9)\n",
    "vggmodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 0.7442, Train Acc: 0.5836, Val Loss: 0.2897, Val Acc: 0.8795\n",
      "Epoch 2 - Train Loss: 0.1983, Train Acc: 0.9362, Val Loss: 0.1698, Val Acc: 0.9157\n",
      "Epoch 3 - Train Loss: 0.1451, Train Acc: 0.9392, Val Loss: 0.1312, Val Acc: 0.9398\n",
      "Epoch 4 - Train Loss: 0.1380, Train Acc: 0.9422, Val Loss: 0.1174, Val Acc: 0.9398\n",
      "Epoch 5 - Train Loss: 0.1455, Train Acc: 0.9392, Val Loss: 0.1100, Val Acc: 0.9398\n",
      "Epoch 6 - Train Loss: 0.1126, Train Acc: 0.9574, Val Loss: 0.1050, Val Acc: 0.9518\n",
      "Epoch 7 - Train Loss: 0.1143, Train Acc: 0.9574, Val Loss: 0.0945, Val Acc: 0.9518\n",
      "Epoch 8 - Train Loss: 0.1246, Train Acc: 0.9514, Val Loss: 0.0897, Val Acc: 0.9518\n",
      "Epoch 9 - Train Loss: 0.1163, Train Acc: 0.9544, Val Loss: 0.0829, Val Acc: 0.9518\n",
      "Epoch 10 - Train Loss: 0.1202, Train Acc: 0.9544, Val Loss: 0.0814, Val Acc: 0.9518\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the combined data\n",
    "vgg_best_accuracy = 0\n",
    "vgg_best_weights = None\n",
    "trl, trac, vall, valac = [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    vggmodel.train()\n",
    "    for i, (inputs, targets) in enumerate(loader_train):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vggmodel(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        train_correct += (predicted == targets).sum().item()\n",
    "        train_total += targets.size(0)\n",
    "    train_loss /= len(dataset_train)\n",
    "    train_acc = train_correct / train_total\n",
    "    trl.append(train_loss)\n",
    "    trac.append(train_acc)\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    vggmodel.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader_valid:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = vggmodel(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            val_correct += (predicted == targets).sum().item()\n",
    "            val_total += targets.size(0)\n",
    "    val_loss /= len(val)\n",
    "    val_acc = val_correct / val_total\n",
    "    vall.append(val_loss)\n",
    "    valac.append(val_acc)\n",
    "\n",
    "    if val_acc > vgg_best_accuracy:\n",
    "        vgg_best_accuracy = val_acc\n",
    "        vgg_best_weights = vggmodel.state_dict()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the test images: 98.07692307692308 %\n",
      "[[73  1]\n",
      " [ 1 29]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99        74\n",
      "           1       0.97      0.97      0.97        30\n",
      "\n",
      "    accuracy                           0.98       104\n",
      "   macro avg       0.98      0.98      0.98       104\n",
      "weighted avg       0.98      0.98      0.98       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cuda_tensor = torch.tensor(vall)\n",
    "vls = cuda_tensor.cpu()\n",
    "cuda_tensor = torch.tensor(trl)\n",
    "tls = cuda_tensor.cpu()\n",
    "\n",
    "vggmodel.load_state_dict(vgg_best_weights)\n",
    "vggmodel.to(device)\n",
    "\n",
    "vggpredict = []\n",
    "vgglabel = []\n",
    "\n",
    "vggmodel.eval()\n",
    "confusion_matrix = torch.zeros(2, 2)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loader_test:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = vggmodel(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        vggpredict.extend(predicted)\n",
    "        vgglabel.extend(labels)\n",
    "\n",
    "        for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "    print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "label_vgg = [tensor.cpu().numpy() for tensor in vgglabel]\n",
    "vgg_array = [tensor.cpu().numpy() for tensor in vggpredict]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(label_vgg, vgg_array))\n",
    "print(classification_report(label_vgg, vgg_array))\n",
    "\n",
    "dfv = pd.DataFrame()\n",
    "dfv[\"vgg\"] = vgg_array\n",
    "dfv[\"label\"] = label_vgg\n",
    "dfv.head()\n",
    "dfv.to_csv('vgwithaug.csv')\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': vggmodel.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, 'checkpointvgg50withaug.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "FGSM_IMAGES = glob.glob(\"adv_examples/fgsm/0_0.png\")\n",
    "fgsm_y = []\n",
    "for img in FGSM_IMAGES:\n",
    "    imgName = img.split(\"/\")[-1]\n",
    "    if imgName.startswith('0_'):\n",
    "        fgsm_y.append(0)\n",
    "    elif imgName.startswith('1_'):\n",
    "        fgsm_y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "images_df = pd.DataFrame()\n",
    "images_df[\"images\"] = FGSM_IMAGES\n",
    "images_df[\"labels\"] = fgsm_y\n",
    "fgsm_test = MyDataset(df_data=images_df, transform=trans_train)\n",
    "loader_fgsm_test = DataLoader(dataset=fgsm_test, batch_size=batch_size//2, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the test images: 100.0 %\n",
      "[[1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sai045/final-btp/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cuda_tensor = torch.tensor(vall)\n",
    "vls = cuda_tensor.cpu()\n",
    "cuda_tensor = torch.tensor(trl)\n",
    "tls = cuda_tensor.cpu()\n",
    "\n",
    "vggmodel.load_state_dict(vgg_best_weights)\n",
    "vggmodel.to(device)\n",
    "\n",
    "vggpredict = []\n",
    "vgglabel = []\n",
    "\n",
    "vggmodel.eval()\n",
    "confusion_matrix = torch.zeros(2, 2)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loader_fgsm_test:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = vggmodel(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        vggpredict.extend(predicted)\n",
    "        vgglabel.extend(labels)\n",
    "\n",
    "        for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "    print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "label_vgg = [tensor.cpu().numpy() for tensor in vgglabel]\n",
    "vgg_array = [tensor.cpu().numpy() for tensor in vggpredict]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(label_vgg, vgg_array))\n",
    "print(classification_report(label_vgg, vgg_array))\n",
    "\n",
    "dfv = pd.DataFrame()\n",
    "dfv[\"vgg\"] = vgg_array\n",
    "dfv[\"label\"] = label_vgg\n",
    "dfv.head()\n",
    "dfv.to_csv('vgwithaug.csv')\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': vggmodel.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, 'checkpointvgg50withaug.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "PGD_IMAGES = glob.glob(\"adv_examples/pgd/*\")\n",
    "pgd_y = []\n",
    "for img in PGD_IMAGES:\n",
    "    imgName = img.split(\"/\")[-1]\n",
    "    if imgName.startswith('0_'):\n",
    "        pgd_y.append(0)\n",
    "    elif imgName.startswith('1_'):\n",
    "        pgd_y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "images_df = pd.DataFrame()\n",
    "images_df[\"images\"] = PGD_IMAGES\n",
    "images_df[\"labels\"] = pgd_y\n",
    "pgd_test = MyDataset(df_data=images_df, transform=trans_train)\n",
    "loader_pgd_test = DataLoader(dataset=pgd_test, batch_size=batch_size//2, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the test images: 98.80952380952381 %\n",
      "[[123   0]\n",
      " [  2  43]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       123\n",
      "           1       1.00      0.96      0.98        45\n",
      "\n",
      "    accuracy                           0.99       168\n",
      "   macro avg       0.99      0.98      0.98       168\n",
      "weighted avg       0.99      0.99      0.99       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cuda_tensor = torch.tensor(vall)\n",
    "vls = cuda_tensor.cpu()\n",
    "cuda_tensor = torch.tensor(trl)\n",
    "tls = cuda_tensor.cpu()\n",
    "\n",
    "vggmodel.load_state_dict(vgg_best_weights)\n",
    "vggmodel.to(device)\n",
    "\n",
    "vggpredict = []\n",
    "vgglabel = []\n",
    "\n",
    "vggmodel.eval()\n",
    "confusion_matrix = torch.zeros(2, 2)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loader_pgd_test:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = vggmodel(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        vggpredict.extend(predicted)\n",
    "        vgglabel.extend(labels)\n",
    "\n",
    "        for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "    print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "label_vgg = [tensor.cpu().numpy() for tensor in vgglabel]\n",
    "vgg_array = [tensor.cpu().numpy() for tensor in vggpredict]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(label_vgg, vgg_array))\n",
    "print(classification_report(label_vgg, vgg_array))\n",
    "\n",
    "dfv = pd.DataFrame()\n",
    "dfv[\"vgg\"] = vgg_array\n",
    "dfv[\"label\"] = label_vgg\n",
    "dfv.head()\n",
    "dfv.to_csv('vgwithaug.csv')\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': vggmodel.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, 'checkpointvgg50withaug.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "I_FGSM_IMAGES = glob.glob(\"adv_examples/ifgsm/*\")\n",
    "ifgsm_y = []\n",
    "for img in I_FGSM_IMAGES:\n",
    "    imgName = img.split(\"/\")[-1]\n",
    "    if imgName.startswith('0_'):\n",
    "        ifgsm_y.append(0)\n",
    "    elif imgName.startswith('1_'):\n",
    "        ifgsm_y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "images_df = pd.DataFrame()\n",
    "images_df[\"images\"] = I_FGSM_IMAGES\n",
    "images_df[\"labels\"] = ifgsm_y\n",
    "ifgsm_test = MyDataset(df_data=images_df, transform=trans_train)\n",
    "loader_i_fgsm_test = DataLoader(dataset=ifgsm_test, batch_size=batch_size//2, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the test images: 100.0 %\n",
      "[[121   0]\n",
      " [  0  58]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       121\n",
      "           1       1.00      1.00      1.00        58\n",
      "\n",
      "    accuracy                           1.00       179\n",
      "   macro avg       1.00      1.00      1.00       179\n",
      "weighted avg       1.00      1.00      1.00       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cuda_tensor = torch.tensor(vall)\n",
    "vls = cuda_tensor.cpu()\n",
    "cuda_tensor = torch.tensor(trl)\n",
    "tls = cuda_tensor.cpu()\n",
    "\n",
    "vggmodel.load_state_dict(vgg_best_weights)\n",
    "vggmodel.to(device)\n",
    "\n",
    "vggpredict = []\n",
    "vgglabel = []\n",
    "\n",
    "vggmodel.eval()\n",
    "confusion_matrix = torch.zeros(2, 2)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loader_i_fgsm_test:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = vggmodel(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        vggpredict.extend(predicted)\n",
    "        vgglabel.extend(labels)\n",
    "\n",
    "        for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "    print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "label_vgg = [tensor.cpu().numpy() for tensor in vgglabel]\n",
    "vgg_array = [tensor.cpu().numpy() for tensor in vggpredict]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(label_vgg, vgg_array))\n",
    "print(classification_report(label_vgg, vgg_array))\n",
    "\n",
    "dfv = pd.DataFrame()\n",
    "dfv[\"vgg\"] = vgg_array\n",
    "dfv[\"label\"] = label_vgg\n",
    "dfv.head()\n",
    "dfv.to_csv('vgwithaug.csv')\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': vggmodel.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, 'checkpointvgg50withaug.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sai045/final-btp/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "safety_checker/model.safetensors not found\n",
      "Fetching 16 files: 100%|██████████| 16/16 [05:26<00:00, 20.43s/it]\n",
      "Loading pipeline components...:  43%|████▎     | 3/7 [00:00<00:01,  3.93it/s]"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained diffusion model\n",
    "diffusion_model = StableDiffusionInpaintPipeline.from_pretrained(\"runwayml/stable-diffusion-inpainting\")\n",
    "diffusion_model.to(device)\n",
    "\n",
    "# Define a function to denoise adversarial examples\n",
    "def denoise_adv_examples(adv_examples, labels):\n",
    "    denoised_examples = []\n",
    "    for adv_img_path, label in zip(adv_examples, labels):\n",
    "        # Load the adversarial image\n",
    "        adv_img = Image.open(adv_img_path)\n",
    "\n",
    "        # Convert the adversarial image to a tensor\n",
    "        transform = transforms.Compose([transforms.ToTensor()])\n",
    "        adv_img_tensor = transform(adv_img)\n",
    "\n",
    "        # Create a mask image (assuming a simple mask for demonstration purposes)\n",
    "        mask_img = Image.new('L', adv_img.size, 255)  # Create a white mask image\n",
    "        mask_img_tensor = transform(mask_img)\n",
    "\n",
    "        # Denoise the adversarial image using the diffusion model\n",
    "        denoised_img = diffusion_model(\"\", adv_img_tensor, mask_image=mask_img_tensor, guidance_scale=7.5)[\"images\"][0]\n",
    "\n",
    "        # Convert the denoised image back to a tensor\n",
    "        denoised_tensor = transform(denoised_img)\n",
    "\n",
    "        denoised_examples.append(denoised_tensor)\n",
    "\n",
    "    return denoised_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoise the FGSM adversarial examples\n",
    "fgsm_denoised_examples = denoise_adv_examples(FGSM_IMAGES, fgsm_y)\n",
    "print(fgsm_denoised_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_tensor = torch.tensor(vall)\n",
    "vls = cuda_tensor.cpu()\n",
    "cuda_tensor = torch.tensor(trl)\n",
    "tls = cuda_tensor.cpu()\n",
    "\n",
    "vggmodel.load_state_dict(vgg_best_weights)\n",
    "vggmodel.to(device)\n",
    "\n",
    "vggpredict = []\n",
    "vgglabel = []\n",
    "\n",
    "vggmodel.eval()\n",
    "confusion_matrix = torch.zeros(2, 2)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in enumerate(fgsm_denoised_examples, fgsm_y):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = vggmodel(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        vggpredict.extend(predicted)\n",
    "        vgglabel.extend(labels)\n",
    "\n",
    "        for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "    print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "label_vgg = [tensor.cpu().numpy() for tensor in vgglabel]\n",
    "vgg_array = [tensor.cpu().numpy() for tensor in vggpredict]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(label_vgg, vgg_array))\n",
    "print(classification_report(label_vgg, vgg_array))\n",
    "\n",
    "dfv = pd.DataFrame()\n",
    "dfv[\"vgg\"] = vgg_array\n",
    "dfv[\"label\"] = label_vgg\n",
    "dfv.head()\n",
    "dfv.to_csv('vgwithaug.csv')\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': vggmodel.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, 'checkpointvgg50withaug.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoise the PGD adversarial examples\n",
    "pgd_denoised_examples = denoise_adv_examples(PGD_IMAGES, pgd_y)\n",
    "print(pgd_denoised_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_tensor = torch.tensor(vall)\n",
    "vls = cuda_tensor.cpu()\n",
    "cuda_tensor = torch.tensor(trl)\n",
    "tls = cuda_tensor.cpu()\n",
    "\n",
    "vggmodel.load_state_dict(vgg_best_weights)\n",
    "vggmodel.to(device)\n",
    "\n",
    "vggpredict = []\n",
    "vgglabel = []\n",
    "\n",
    "vggmodel.eval()\n",
    "confusion_matrix = torch.zeros(2, 2)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in enumerate(pgd_denoised_examples, pgd_y):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = vggmodel(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        vggpredict.extend(predicted)\n",
    "        vgglabel.extend(labels)\n",
    "\n",
    "        for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "    print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "label_vgg = [tensor.cpu().numpy() for tensor in vgglabel]\n",
    "vgg_array = [tensor.cpu().numpy() for tensor in vggpredict]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(label_vgg, vgg_array))\n",
    "print(classification_report(label_vgg, vgg_array))\n",
    "\n",
    "dfv = pd.DataFrame()\n",
    "dfv[\"vgg\"] = vgg_array\n",
    "dfv[\"label\"] = label_vgg\n",
    "dfv.head()\n",
    "dfv.to_csv('vgwithaug.csv')\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': vggmodel.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, 'checkpointvgg50withaug.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoise the I-FGSM adversarial examples\n",
    "ifgsm_denoised_examples = denoise_adv_examples(I_FGSM_IMAGES, ifgsm_y)\n",
    "print(ifgsm_denoised_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_tensor = torch.tensor(vall)\n",
    "vls = cuda_tensor.cpu()\n",
    "cuda_tensor = torch.tensor(trl)\n",
    "tls = cuda_tensor.cpu()\n",
    "\n",
    "vggmodel.load_state_dict(vgg_best_weights)\n",
    "vggmodel.to(device)\n",
    "\n",
    "vggpredict = []\n",
    "vgglabel = []\n",
    "\n",
    "vggmodel.eval()\n",
    "confusion_matrix = torch.zeros(2, 2)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in enumerate(ifgsm_denoised_examples, ifgsm_y):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = vggmodel(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        vggpredict.extend(predicted)\n",
    "        vgglabel.extend(labels)\n",
    "\n",
    "        for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "    print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "label_vgg = [tensor.cpu().numpy() for tensor in vgglabel]\n",
    "vgg_array = [tensor.cpu().numpy() for tensor in vggpredict]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(label_vgg, vgg_array))\n",
    "print(classification_report(label_vgg, vgg_array))\n",
    "\n",
    "dfv = pd.DataFrame()\n",
    "dfv[\"vgg\"] = vgg_array\n",
    "dfv[\"label\"] = label_vgg\n",
    "dfv.head()\n",
    "dfv.to_csv('vgwithaug.csv')\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': vggmodel.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, 'checkpointvgg50withaug.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
